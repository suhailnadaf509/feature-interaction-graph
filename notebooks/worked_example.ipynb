{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37799181",
   "metadata": {},
   "source": [
    "# Feature Interaction Graph — Worked Example\n",
    "\n",
    "This notebook demonstrates the full pipeline for constructing a feature interaction\n",
    "graph on **GPT-2-small** with community SAEs from SAELens.\n",
    "\n",
    "We'll walk through:\n",
    "1. Loading the model and SAEs\n",
    "2. Collecting feature activations\n",
    "3. Building the co-activation atlas\n",
    "4. Identifying candidate interaction pairs\n",
    "5. Measuring causal interactions\n",
    "6. Building and analyzing the interaction graph\n",
    "7. Extracting behavior-specific subgraphs\n",
    "8. Predicting and visualizing steering cascades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e036a91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# !pip install -e \".[dev]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee18329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path\n",
    "\n",
    "from feature_graph.config import Config\n",
    "from feature_graph.utils import setup_logging, set_seed\n",
    "\n",
    "setup_logging(\"INFO\")\n",
    "set_seed(42)\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9010944",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "We configure a small-scale run for demonstration. For a full research run,\n",
    "increase `n_tokens` to 1M+ and `top_k_features` to 200+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039c56b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config(\n",
    "    model_name=\"gpt2-small\",\n",
    "    sae_release=\"gpt2-small-res-jb\",\n",
    "    sae_id_template=\"blocks.{layer}.hook_resid_post\",\n",
    "    layers=[0, 1, 2, 3, 4, 5],  # First 6 layers for demo\n",
    "    device=device,\n",
    "\n",
    "    # Scale down for notebook demo\n",
    "    n_tokens=10_000,\n",
    "    context_length=128,\n",
    "    batch_size=8,\n",
    "\n",
    "    # Candidate filtering\n",
    "    top_k_features=50,  # Small for demo\n",
    "    layer_window=2,\n",
    "\n",
    "    # Interaction measurement\n",
    "    interaction_method=\"clamping\",\n",
    "    n_intervention_samples=20,  # Small for demo\n",
    "\n",
    "    # Statistical testing\n",
    "    significance_level=0.05,\n",
    "    correction_method=\"fdr_bh\",\n",
    "\n",
    "    output_dir=\"outputs/demo\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Save config for reproducibility\n",
    "cfg.save(\"outputs/demo/config.json\")\n",
    "print(f\"Config saved. Analyzing layers {cfg.layers} with top-{cfg.top_k_features} features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea348323",
   "metadata": {},
   "source": [
    "## 2. Load Model and SAEs\n",
    "\n",
    "We use TransformerLens for the model and SAELens for the sparse autoencoders.\n",
    "On first run, this will download the model and SAE weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa48021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_graph.loading import load_model_and_saes, get_sae_dict_size\n",
    "\n",
    "model, saes = load_model_and_saes(cfg)\n",
    "\n",
    "print(f\"\\nModel: {cfg.model_name}\")\n",
    "print(f\"d_model: {model.cfg.d_model}\")\n",
    "print(f\"n_layers: {model.cfg.n_layers}\")\n",
    "print(f\"\\nLoaded SAEs for layers: {sorted(saes.keys())}\")\n",
    "for layer in sorted(saes.keys()):\n",
    "    print(f\"  Layer {layer}: {get_sae_dict_size(saes[layer])} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff58e48",
   "metadata": {},
   "source": [
    "## 3. Collect Feature Activations\n",
    "\n",
    "We run a corpus of text through the model and record SAE feature activations\n",
    "at every layer. This gives us the raw data for co-activation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9fe4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_graph.activations import collect_activations\n",
    "\n",
    "activation_store = collect_activations(model, saes, cfg)\n",
    "\n",
    "print(f\"\\nCollected activations for {activation_store.n_tokens} tokens\")\n",
    "print(f\"Layers: {activation_store.layers}\")\n",
    "for layer in activation_store.layers:\n",
    "    acts = activation_store.activations[layer]\n",
    "    freq = activation_store.feature_frequencies[layer]\n",
    "    n_active = (freq > 0).sum()\n",
    "    mean_freq = freq[freq > 0].mean() if n_active > 0 else 0\n",
    "    print(f\"  Layer {layer}: {acts.shape[1]} features, {n_active} active, mean freq={mean_freq:.4f}\")\n",
    "\n",
    "# Save activations\n",
    "activation_store.save(\"outputs/demo/activations.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759532e6",
   "metadata": {},
   "source": [
    "## 4. Build Co-Activation Atlas\n",
    "\n",
    "The co-activation atlas is the correlational scaffold. For each pair of features\n",
    "across layers, we compute:\n",
    "- **Co-activation frequency**: P(f_j > 0 | f_i > 0)\n",
    "- **Pointwise mutual information**: log P(i,j) / (P(i) · P(j))\n",
    "- **Co-activation ratio**: P(f_j > 0 | f_i > 0) / P(f_j > 0)\n",
    "\n",
    "This tells us which pairs *might* interact — but correlation ≠ causation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed83c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_graph.coactivation import build_coactivation_atlas\n",
    "\n",
    "atlas = build_coactivation_atlas(activation_store, cfg)\n",
    "\n",
    "print(f\"\\nCo-activation atlas built for {len(atlas.layer_pairs)} layer pairs:\")\n",
    "for lp in atlas.layer_pairs:\n",
    "    pmi_mat = atlas.pmi.get(lp)\n",
    "    if pmi_mat is not None:\n",
    "        n_nonzero = pmi_mat.nnz\n",
    "        total = pmi_mat.shape[0] * pmi_mat.shape[1]\n",
    "        print(f\"  {lp[0]} → {lp[1]}: {n_nonzero} / {total} pairs retained ({100*n_nonzero/total:.2f}%)\")\n",
    "\n",
    "atlas.save(\"outputs/demo/atlas.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6585c0a",
   "metadata": {},
   "source": [
    "## 5. Identify Candidate Pairs\n",
    "\n",
    "The four-stage pruning pipeline:\n",
    "1. **Importance filter**: Select top-K features by activation frequency × causal effect\n",
    "2. **Co-activation filter**: Remove pairs with negligible co-activation\n",
    "3. **Decoder alignment filter**: Remove pairs with near-zero direct alignment (adjacent layers)\n",
    "4. **Layer locality**: Only test pairs within the layer window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759cebf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_graph.candidates import identify_candidates\n",
    "\n",
    "candidates = identify_candidates(\n",
    "    atlas, saes, cfg,\n",
    "    activation_store=activation_store,\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "print(f\"\\n{len(candidates)} candidate pairs identified\")\n",
    "\n",
    "# Show distribution across layer pairs\n",
    "from collections import Counter\n",
    "pair_counts = Counter((c.src.layer, c.tgt.layer) for c in candidates)\n",
    "print(\"\\nCandidates by layer pair:\")\n",
    "for (l_src, l_tgt), count in sorted(pair_counts.items()):\n",
    "    print(f\"  Layer {l_src} → {l_tgt}: {count} pairs\")\n",
    "\n",
    "# Show some candidate pairs\n",
    "print(\"\\nTop 10 candidate pairs (by PMI):\")\n",
    "for i, c in enumerate(candidates[:10]):\n",
    "    print(f\"  {c.src_id} → {c.tgt_id} | PMI={c.pmi:.2f} | coact={c.coactivation_prob:.3f} | align={c.decoder_alignment:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54b6553",
   "metadata": {},
   "source": [
    "## 6. Measure Causal Interactions\n",
    "\n",
    "This is the core scientific step. For each candidate pair, we:\n",
    "1. Sample inputs where the source feature is active\n",
    "2. Clamp the source feature to high/low values\n",
    "3. Measure the effect on the target feature\n",
    "4. Compute interaction strength with statistical testing\n",
    "\n",
    "**This step takes the most time.** For the demo config (20 samples × small candidate set), it should take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941341bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_graph.interactions import measure_interactions\n",
    "\n",
    "interactions = measure_interactions(model, saes, candidates, cfg)\n",
    "\n",
    "print(f\"\\n{len(interactions)} interactions measured\")\n",
    "\n",
    "# Count by type\n",
    "type_counts = Counter(r.interaction_type for r in interactions)\n",
    "print(f\"\\nInteraction types:\")\n",
    "for t, c in type_counts.items():\n",
    "    print(f\"  {t}: {c}\")\n",
    "\n",
    "# Count significant interactions\n",
    "sig = [r for r in interactions if r.p_value < cfg.significance_level]\n",
    "print(f\"\\n{len(sig)} significant interactions (p < {cfg.significance_level})\")\n",
    "\n",
    "# Show strongest interactions\n",
    "sig.sort(key=lambda r: r.abs_strength, reverse=True)\n",
    "print(\"\\nTop 10 strongest significant interactions:\")\n",
    "for r in sig[:10]:\n",
    "    print(f\"  {r.src_id} → {r.tgt_id} | {r.interaction_type} | strength={r.mean_strength:.4f} | p={r.p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a464efed",
   "metadata": {},
   "source": [
    "## 7. Build and Analyze the Interaction Graph\n",
    "\n",
    "We now construct the NetworkX graph from the significant interactions\n",
    "and compute graph-theoretic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb59fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_graph.graph import build_interaction_graph, save_graph\n",
    "from feature_graph.analysis import compute_graph_statistics, find_hubs, count_motifs, detect_communities\n",
    "\n",
    "G = build_interaction_graph(interactions, cfg)\n",
    "\n",
    "# Save in multiple formats\n",
    "save_graph(G, \"outputs/demo/graph.graphml\", format=\"graphml\")\n",
    "save_graph(G, \"outputs/demo/graph.json\", format=\"json\")\n",
    "\n",
    "# Compute statistics\n",
    "stats = compute_graph_statistics(G)\n",
    "print(stats.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b72abcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hub identification\n",
    "hubs = find_hubs(G, top_k=10, metric=\"total_degree\")\n",
    "print(\"\\nTop 10 Hub Features (by total degree):\")\n",
    "for h in hubs:\n",
    "    print(f\"  {h['id']}: degree={h.get('total_degree_score', 0)}, \"\n",
    "          f\"in={h['in_degree']}, out={h['out_degree']}, label='{h['label']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d38255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Community detection\n",
    "communities = detect_communities(G)\n",
    "print(f\"\\nDetected {len(communities)} communities\")\n",
    "for i, comm in enumerate(sorted(communities, key=len, reverse=True)[:5]):\n",
    "    print(f\"  Community {i}: {len(comm)} features\")\n",
    "    # Show a few features from each\n",
    "    for fid in list(comm)[:3]:\n",
    "        layer = G.nodes[fid].get('layer', '?')\n",
    "        label = G.nodes[fid].get('label', '')\n",
    "        print(f\"    {fid} (layer {layer}): {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f20b62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Motif counting\n",
    "motifs = count_motifs(G)\n",
    "print(\"\\nInteraction Motifs:\")\n",
    "for motif, count in motifs.items():\n",
    "    print(f\"  {motif}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eada0b9a",
   "metadata": {},
   "source": [
    "## 8. Visualization\n",
    "\n",
    "Generate interactive visualizations of the interaction graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c64fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_graph.visualization import (\n",
    "    render_interactive_graph,\n",
    "    render_neighborhood,\n",
    "    plot_degree_distribution,\n",
    "    plot_layer_distance_decay,\n",
    "    plot_interaction_type_breakdown,\n",
    ")\n",
    "\n",
    "# Full graph visualization\n",
    "render_interactive_graph(G, \"outputs/demo/full_graph.html\")\n",
    "print(\"Full graph visualization saved to outputs/demo/full_graph.html\")\n",
    "\n",
    "# Statistical plots\n",
    "fig_degree = plot_degree_distribution(G)\n",
    "if fig_degree:\n",
    "    fig_degree.show()\n",
    "\n",
    "fig_decay = plot_layer_distance_decay(G)\n",
    "if fig_decay:\n",
    "    fig_decay.show()\n",
    "\n",
    "fig_types = plot_interaction_type_breakdown(G)\n",
    "if fig_types:\n",
    "    fig_types.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f493faf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neighborhood exploration for the top hub\n",
    "if hubs:\n",
    "    top_hub = hubs[0]['id']\n",
    "    render_neighborhood(G, top_hub, n_hops=1, output_path=\"outputs/demo/top_hub_neighborhood.html\")\n",
    "    print(f\"Neighborhood of {top_hub} saved to outputs/demo/top_hub_neighborhood.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4916f2",
   "metadata": {},
   "source": [
    "## 9. Behavior-Specific Subgraph Extraction\n",
    "\n",
    "We extract the subgraph relevant to factual recall behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee7c996",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_graph.subgraphs import extract_behavior_subgraph, PREDEFINED_BEHAVIORS\n",
    "\n",
    "# Extract factual recall subgraph\n",
    "factual_result = extract_behavior_subgraph(\n",
    "    model, saes, G,\n",
    "    behavior=\"factual_recall\",\n",
    "    cfg=cfg,\n",
    "    n_hops=2,\n",
    "    top_k_seeds=10,\n",
    ")\n",
    "\n",
    "print(f\"\\nFactual Recall Subgraph:\")\n",
    "print(f\"  Seed features: {len(factual_result.seed_features)}\")\n",
    "print(f\"  Subgraph nodes: {factual_result.subgraph.number_of_nodes()}\")\n",
    "print(f\"  Subgraph edges: {factual_result.subgraph.number_of_edges()}\")\n",
    "\n",
    "print(\"\\nTop seed features:\")\n",
    "for sf in factual_result.seed_features[:10]:\n",
    "    print(f\"  {sf['feature_id']}: effect_size={sf['effect_size']:.4f}\")\n",
    "\n",
    "# Visualize\n",
    "if factual_result.subgraph.number_of_nodes() > 0:\n",
    "    render_interactive_graph(\n",
    "        factual_result.subgraph,\n",
    "        \"outputs/demo/factual_recall_subgraph.html\",\n",
    "        title=\"Factual Recall Circuit\",\n",
    "        seed_nodes=[sf['feature_id'] for sf in factual_result.seed_features[:10]],\n",
    "    )\n",
    "    print(\"\\nVisualization saved to outputs/demo/factual_recall_subgraph.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a767cbdc",
   "metadata": {},
   "source": [
    "## 10. Steering Cascade Prediction\n",
    "\n",
    "Use the interaction graph to predict what happens when we steer on a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c61155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_graph.steering import predict_cascade\n",
    "from feature_graph.visualization import render_cascade_overlay\n",
    "\n",
    "# Predict cascade from the top hub feature\n",
    "if hubs:\n",
    "    top_hub = hubs[0]['id']\n",
    "    cascade = predict_cascade(G, top_hub, steer_delta=2.0, n_hops=2)\n",
    "    print(cascade.summary())\n",
    "\n",
    "    # Visualize cascade\n",
    "    render_cascade_overlay(G, cascade, \"outputs/demo/cascade.html\")\n",
    "    print(\"\\nCascade visualization saved to outputs/demo/cascade.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04b9cfc",
   "metadata": {},
   "source": [
    "## 11. Summary and Next Steps\n",
    "\n",
    "This demo showed the complete pipeline on a small scale. For a full research run:\n",
    "\n",
    "1. **Increase `n_tokens`** to 1M+ for robust co-activation statistics\n",
    "2. **Increase `top_k_features`** to 200+ for a richer graph\n",
    "3. **Increase `n_intervention_samples`** to 100+ for reliable causal estimates\n",
    "4. **Analyze all layers** (set `layers=list(range(12))` for GPT-2-small)\n",
    "5. **Compare methods**: run with `interaction_method='both'` to compare clamping vs. Jacobian\n",
    "6. **Extract multiple behavior subgraphs** and compare their overlap\n",
    "7. **Test steering predictions** by comparing predicted vs. actual cascading effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f6b917",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Pipeline complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nOutputs saved to: outputs/demo/\")\n",
    "print(f\"  config.json          - Run configuration\")\n",
    "print(f\"  activations.h5       - Feature activations\")\n",
    "print(f\"  atlas.h5             - Co-activation atlas\")\n",
    "print(f\"  graph.graphml        - Interaction graph (GraphML)\")\n",
    "print(f\"  graph.json           - Interaction graph (JSON)\")\n",
    "print(f\"  full_graph.html      - Interactive graph visualization\")\n",
    "print(f\"  top_hub_neighborhood.html - Hub neighborhood\")\n",
    "print(f\"  factual_recall_subgraph.html - Factual recall circuit\")\n",
    "print(f\"  cascade.html         - Steering cascade prediction\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
